{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/data_normalized'\n",
    "data_set = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CA1_17419014_resampled.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join(DATA_DIR, data_set[10])\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IeegDataset(Dataset):\n",
    "    def __init__(self, data_dir, seq_length=3035):\n",
    "        self.data_dir = data_dir\n",
    "        self.signals = os.listdir(self.data_dir)\n",
    "        self.seq_length = seq_length\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.classes = [f.split('_')[0] for f in self.signals]\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.classes)\n",
    "\n",
    "        for file in self.signals:\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            for column in df.columns: \n",
    "                signal_data = df[column].values[:self.seq_length]\n",
    "                if len(signal_data) < self.seq_length:\n",
    "                    # Padding if necesary \n",
    "                    signal_data = np.pad(signal_data, (0, self.seq_length - len(signal_data)), 'constant')\n",
    "    \n",
    "                class_label = self.label_encoder.transform([file.split('_')[0]])\n",
    "                self.data.append(signal_data)\n",
    "                self.labels.append(class_label)\n",
    "        \n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    def __getitem__(self, index) -> torch.tensor:\n",
    "        return self.data[index] , self.labels[index]\n",
    "    def get_class_mapping(self):\n",
    "        return {i: class_name for i, class_name in enumerate(self.label_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/1254916981.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  self.data = torch.tensor(self.data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = IeegDataset('../data/data_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_size = 3035  # sequence length\n",
    "num_classes = len(dataset.label_encoder.classes_)\n",
    "model = SimpleNN(input_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking uri: http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "print('tracking uri:', mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"IEEG_Classification_Baseline\"\n",
    "\n",
    "# Create a new experiment or set the existing one\n",
    "mlflow.set_experiment(experiment_name)\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs=200):\n",
    "    # Start a new run\n",
    "    mlflow.pytorch.autolog()\n",
    "    with mlflow.start_run(run_name=\"ANN-Baseline\") as run:\n",
    "        # Log parameters\n",
    "        # mlflow.log_param(\"epochs\", num_epochs)\n",
    "        # mlflow.log_param(\"batch_size\", batch_size)\n",
    "        # mlflow.log_param(\"learning_rate\", 0.001)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move to GPU\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.squeeze())  # Squeeze to remove extra dimension\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            # Log loss\n",
    "            # mlflow.log_metric(\"loss\", avg_loss, step=epoch)\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move to GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.squeeze()).sum().item()  # Squeeze to remove extra dimension\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "        \n",
    "        # # Log accuracy\n",
    "        # mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        # # Log the model\n",
    "        # mlflow.pytorch.log_model(model, \"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 19:47:40 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0196\n",
      "Epoch [2/200], Loss: 0.0295\n",
      "Epoch [3/200], Loss: 0.0153\n",
      "Epoch [4/200], Loss: 0.0235\n",
      "Epoch [5/200], Loss: 0.0203\n",
      "Epoch [6/200], Loss: 0.0194\n",
      "Epoch [7/200], Loss: 0.0231\n",
      "Epoch [8/200], Loss: 0.0206\n",
      "Epoch [9/200], Loss: 0.0196\n",
      "Epoch [10/200], Loss: 0.0254\n",
      "Epoch [11/200], Loss: 0.0155\n",
      "Epoch [12/200], Loss: 0.0108\n",
      "Epoch [13/200], Loss: 0.0180\n",
      "Epoch [14/200], Loss: 0.0113\n",
      "Epoch [15/200], Loss: 0.0203\n",
      "Epoch [16/200], Loss: 0.0172\n",
      "Epoch [17/200], Loss: 0.0236\n",
      "Epoch [18/200], Loss: 0.0218\n",
      "Epoch [19/200], Loss: 0.0175\n",
      "Epoch [20/200], Loss: 0.0141\n",
      "Epoch [21/200], Loss: 0.0190\n",
      "Epoch [22/200], Loss: 0.0367\n",
      "Epoch [23/200], Loss: 0.0273\n",
      "Epoch [24/200], Loss: 0.0311\n",
      "Epoch [25/200], Loss: 0.0208\n",
      "Epoch [26/200], Loss: 0.0290\n",
      "Epoch [27/200], Loss: 0.0364\n",
      "Epoch [28/200], Loss: 0.0316\n",
      "Epoch [29/200], Loss: 0.0218\n",
      "Epoch [30/200], Loss: 0.0306\n",
      "Epoch [31/200], Loss: 0.0249\n",
      "Epoch [32/200], Loss: 0.0210\n",
      "Epoch [33/200], Loss: 0.0220\n",
      "Epoch [34/200], Loss: 0.0247\n",
      "Epoch [35/200], Loss: 0.0126\n",
      "Epoch [36/200], Loss: 0.0280\n",
      "Epoch [37/200], Loss: 0.0202\n",
      "Epoch [38/200], Loss: 0.0272\n",
      "Epoch [39/200], Loss: 0.0218\n",
      "Epoch [40/200], Loss: 0.0190\n",
      "Epoch [41/200], Loss: 0.0109\n",
      "Epoch [42/200], Loss: 0.0131\n",
      "Epoch [43/200], Loss: 0.0174\n",
      "Epoch [44/200], Loss: 0.0139\n",
      "Epoch [45/200], Loss: 0.0111\n",
      "Epoch [46/200], Loss: 0.0143\n",
      "Epoch [47/200], Loss: 0.0079\n",
      "Epoch [48/200], Loss: 0.0119\n",
      "Epoch [49/200], Loss: 0.0152\n",
      "Epoch [50/200], Loss: 0.0126\n",
      "Epoch [51/200], Loss: 0.0107\n",
      "Epoch [52/200], Loss: 0.0258\n",
      "Epoch [53/200], Loss: 0.0268\n",
      "Epoch [54/200], Loss: 0.0163\n",
      "Epoch [55/200], Loss: 0.0168\n",
      "Epoch [56/200], Loss: 0.0232\n",
      "Epoch [57/200], Loss: 0.0246\n",
      "Epoch [58/200], Loss: 0.0171\n",
      "Epoch [59/200], Loss: 0.0190\n",
      "Epoch [60/200], Loss: 0.0334\n",
      "Epoch [61/200], Loss: 0.0201\n",
      "Epoch [62/200], Loss: 0.0224\n",
      "Epoch [63/200], Loss: 0.0258\n",
      "Epoch [64/200], Loss: 0.0218\n",
      "Epoch [65/200], Loss: 0.0225\n",
      "Epoch [66/200], Loss: 0.0256\n",
      "Epoch [67/200], Loss: 0.0154\n",
      "Epoch [68/200], Loss: 0.0345\n",
      "Epoch [69/200], Loss: 0.0306\n",
      "Epoch [70/200], Loss: 0.0269\n",
      "Epoch [71/200], Loss: 0.0147\n",
      "Epoch [72/200], Loss: 0.0223\n",
      "Epoch [73/200], Loss: 0.0164\n",
      "Epoch [74/200], Loss: 0.0151\n",
      "Epoch [75/200], Loss: 0.0228\n",
      "Epoch [76/200], Loss: 0.0219\n",
      "Epoch [77/200], Loss: 0.0067\n",
      "Epoch [78/200], Loss: 0.0260\n",
      "Epoch [79/200], Loss: 0.0180\n",
      "Epoch [80/200], Loss: 0.0166\n",
      "Epoch [81/200], Loss: 0.0152\n",
      "Epoch [82/200], Loss: 0.0201\n",
      "Epoch [83/200], Loss: 0.0130\n",
      "Epoch [84/200], Loss: 0.0190\n",
      "Epoch [85/200], Loss: 0.0136\n",
      "Epoch [86/200], Loss: 0.0212\n",
      "Epoch [87/200], Loss: 0.0098\n",
      "Epoch [88/200], Loss: 0.0067\n",
      "Epoch [89/200], Loss: 0.0252\n",
      "Epoch [90/200], Loss: 0.0066\n",
      "Epoch [91/200], Loss: 0.0204\n",
      "Epoch [92/200], Loss: 0.0113\n",
      "Epoch [93/200], Loss: 0.0276\n",
      "Epoch [94/200], Loss: 0.0220\n",
      "Epoch [95/200], Loss: 0.0203\n",
      "Epoch [96/200], Loss: 0.0212\n",
      "Epoch [97/200], Loss: 0.0185\n",
      "Epoch [98/200], Loss: 0.0294\n",
      "Epoch [99/200], Loss: 0.0129\n",
      "Epoch [100/200], Loss: 0.0116\n",
      "Epoch [101/200], Loss: 0.0261\n",
      "Epoch [102/200], Loss: 0.0087\n",
      "Epoch [103/200], Loss: 0.0159\n",
      "Epoch [104/200], Loss: 0.0128\n",
      "Epoch [105/200], Loss: 0.0296\n",
      "Epoch [106/200], Loss: 0.0181\n",
      "Epoch [107/200], Loss: 0.0285\n",
      "Epoch [108/200], Loss: 0.0176\n",
      "Epoch [109/200], Loss: 0.0313\n",
      "Epoch [110/200], Loss: 0.0124\n",
      "Epoch [111/200], Loss: 0.0162\n",
      "Epoch [112/200], Loss: 0.0123\n",
      "Epoch [113/200], Loss: 0.0183\n",
      "Epoch [114/200], Loss: 0.0177\n",
      "Epoch [115/200], Loss: 0.0167\n",
      "Epoch [116/200], Loss: 0.0151\n",
      "Epoch [117/200], Loss: 0.0211\n",
      "Epoch [118/200], Loss: 0.0121\n",
      "Epoch [119/200], Loss: 0.0061\n",
      "Epoch [120/200], Loss: 0.0146\n",
      "Epoch [121/200], Loss: 0.0193\n",
      "Epoch [122/200], Loss: 0.0144\n",
      "Epoch [123/200], Loss: 0.0319\n",
      "Epoch [124/200], Loss: 0.0229\n",
      "Epoch [125/200], Loss: 0.0274\n",
      "Epoch [126/200], Loss: 0.0336\n",
      "Epoch [127/200], Loss: 0.0256\n",
      "Epoch [128/200], Loss: 0.0135\n",
      "Epoch [129/200], Loss: 0.0217\n",
      "Epoch [130/200], Loss: 0.0099\n",
      "Epoch [131/200], Loss: 0.0129\n",
      "Epoch [132/200], Loss: 0.0229\n",
      "Epoch [133/200], Loss: 0.0151\n",
      "Epoch [134/200], Loss: 0.0251\n",
      "Epoch [135/200], Loss: 0.0368\n",
      "Epoch [136/200], Loss: 0.0179\n",
      "Epoch [137/200], Loss: 0.0199\n",
      "Epoch [138/200], Loss: 0.0122\n",
      "Epoch [139/200], Loss: 0.0206\n",
      "Epoch [140/200], Loss: 0.0419\n",
      "Epoch [141/200], Loss: 0.0113\n",
      "Epoch [142/200], Loss: 0.0220\n",
      "Epoch [143/200], Loss: 0.0179\n",
      "Epoch [144/200], Loss: 0.0130\n",
      "Epoch [145/200], Loss: 0.0277\n",
      "Epoch [146/200], Loss: 0.0152\n",
      "Epoch [147/200], Loss: 0.0133\n",
      "Epoch [148/200], Loss: 0.0149\n",
      "Epoch [149/200], Loss: 0.0112\n",
      "Epoch [150/200], Loss: 0.0313\n",
      "Epoch [151/200], Loss: 0.0291\n",
      "Epoch [152/200], Loss: 0.0261\n",
      "Epoch [153/200], Loss: 0.0120\n",
      "Epoch [154/200], Loss: 0.0230\n",
      "Epoch [155/200], Loss: 0.0119\n",
      "Epoch [156/200], Loss: 0.0200\n",
      "Epoch [157/200], Loss: 0.0224\n",
      "Epoch [158/200], Loss: 0.0194\n",
      "Epoch [159/200], Loss: 0.0159\n",
      "Epoch [160/200], Loss: 0.0049\n",
      "Epoch [161/200], Loss: 0.0158\n",
      "Epoch [162/200], Loss: 0.0079\n",
      "Epoch [163/200], Loss: 0.0072\n",
      "Epoch [164/200], Loss: 0.0164\n",
      "Epoch [165/200], Loss: 0.0139\n",
      "Epoch [166/200], Loss: 0.0147\n",
      "Epoch [167/200], Loss: 0.0150\n",
      "Epoch [168/200], Loss: 0.0149\n",
      "Epoch [169/200], Loss: 0.0168\n",
      "Epoch [170/200], Loss: 0.0154\n",
      "Epoch [171/200], Loss: 0.0265\n",
      "Epoch [172/200], Loss: 0.0216\n",
      "Epoch [173/200], Loss: 0.0209\n",
      "Epoch [174/200], Loss: 0.0157\n",
      "Epoch [175/200], Loss: 0.0359\n",
      "Epoch [176/200], Loss: 0.0126\n",
      "Epoch [177/200], Loss: 0.0151\n",
      "Epoch [178/200], Loss: 0.0067\n",
      "Epoch [179/200], Loss: 0.0104\n",
      "Epoch [180/200], Loss: 0.0127\n",
      "Epoch [181/200], Loss: 0.0128\n",
      "Epoch [182/200], Loss: 0.0067\n",
      "Epoch [183/200], Loss: 0.0147\n",
      "Epoch [184/200], Loss: 0.0192\n",
      "Epoch [185/200], Loss: 0.0166\n",
      "Epoch [186/200], Loss: 0.0095\n",
      "Epoch [187/200], Loss: 0.0262\n",
      "Epoch [188/200], Loss: 0.0142\n",
      "Epoch [189/200], Loss: 0.0261\n",
      "Epoch [190/200], Loss: 0.0191\n",
      "Epoch [191/200], Loss: 0.0073\n",
      "Epoch [192/200], Loss: 0.0211\n",
      "Epoch [193/200], Loss: 0.0176\n",
      "Epoch [194/200], Loss: 0.0123\n",
      "Epoch [195/200], Loss: 0.0075\n",
      "Epoch [196/200], Loss: 0.0122\n",
      "Epoch [197/200], Loss: 0.0173\n",
      "Epoch [198/200], Loss: 0.0142\n",
      "Epoch [199/200], Loss: 0.0090\n",
      "Epoch [200/200], Loss: 0.0279\n",
      "Accuracy of the model on the test data: 88.95%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model while tracking with MLflow\n",
    "train_and_evaluate(model, train_loader, test_loader, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1699, Accuracy: 0.5286, Precision: 0.5349, Recall: 0.5286, F1 Score: 0.4853\n",
      "Epoch [2/200], Loss: 0.7733, Accuracy: 0.7110, Precision: 0.7181, Recall: 0.7110, F1 Score: 0.6799\n",
      "Epoch [3/200], Loss: 0.5675, Accuracy: 0.8057, Precision: 0.8043, Recall: 0.8057, F1 Score: 0.7986\n",
      "Epoch [4/200], Loss: 0.4368, Accuracy: 0.8538, Precision: 0.8523, Recall: 0.8538, F1 Score: 0.8519\n",
      "Epoch [5/200], Loss: 0.3408, Accuracy: 0.8824, Precision: 0.8819, Recall: 0.8824, F1 Score: 0.8813\n",
      "Epoch [6/200], Loss: 0.3002, Accuracy: 0.9038, Precision: 0.9033, Recall: 0.9038, F1 Score: 0.9031\n",
      "Epoch [7/200], Loss: 0.2571, Accuracy: 0.9100, Precision: 0.9095, Recall: 0.9100, F1 Score: 0.9094\n",
      "Epoch [8/200], Loss: 0.2026, Accuracy: 0.9295, Precision: 0.9291, Recall: 0.9295, F1 Score: 0.9292\n",
      "Epoch [9/200], Loss: 0.1869, Accuracy: 0.9424, Precision: 0.9423, Recall: 0.9424, F1 Score: 0.9423\n",
      "Epoch [10/200], Loss: 0.1748, Accuracy: 0.9433, Precision: 0.9431, Recall: 0.9433, F1 Score: 0.9429\n",
      "Epoch [11/200], Loss: 0.1536, Accuracy: 0.9481, Precision: 0.9480, Recall: 0.9481, F1 Score: 0.9479\n",
      "Epoch [12/200], Loss: 0.1369, Accuracy: 0.9505, Precision: 0.9505, Recall: 0.9505, F1 Score: 0.9504\n",
      "Epoch [13/200], Loss: 0.1344, Accuracy: 0.9571, Precision: 0.9571, Recall: 0.9571, F1 Score: 0.9570\n",
      "Epoch [14/200], Loss: 0.1141, Accuracy: 0.9571, Precision: 0.9571, Recall: 0.9571, F1 Score: 0.9571\n",
      "Epoch [15/200], Loss: 0.1216, Accuracy: 0.9605, Precision: 0.9604, Recall: 0.9605, F1 Score: 0.9604\n",
      "Epoch [16/200], Loss: 0.1097, Accuracy: 0.9633, Precision: 0.9633, Recall: 0.9633, F1 Score: 0.9633\n",
      "Epoch [17/200], Loss: 0.1154, Accuracy: 0.9667, Precision: 0.9665, Recall: 0.9667, F1 Score: 0.9666\n",
      "Epoch [18/200], Loss: 0.0923, Accuracy: 0.9705, Precision: 0.9706, Recall: 0.9705, F1 Score: 0.9705\n",
      "Epoch [19/200], Loss: 0.1032, Accuracy: 0.9662, Precision: 0.9662, Recall: 0.9662, F1 Score: 0.9661\n",
      "Epoch [20/200], Loss: 0.0742, Accuracy: 0.9762, Precision: 0.9762, Recall: 0.9762, F1 Score: 0.9761\n",
      "Epoch [21/200], Loss: 0.0739, Accuracy: 0.9757, Precision: 0.9758, Recall: 0.9757, F1 Score: 0.9757\n",
      "Epoch [22/200], Loss: 0.0746, Accuracy: 0.9738, Precision: 0.9737, Recall: 0.9738, F1 Score: 0.9737\n",
      "Epoch [23/200], Loss: 0.0673, Accuracy: 0.9805, Precision: 0.9805, Recall: 0.9805, F1 Score: 0.9805\n",
      "Epoch [24/200], Loss: 0.0730, Accuracy: 0.9762, Precision: 0.9762, Recall: 0.9762, F1 Score: 0.9761\n",
      "Epoch [25/200], Loss: 0.0731, Accuracy: 0.9748, Precision: 0.9748, Recall: 0.9748, F1 Score: 0.9747\n",
      "Epoch [26/200], Loss: 0.0624, Accuracy: 0.9800, Precision: 0.9801, Recall: 0.9800, F1 Score: 0.9800\n",
      "Epoch [27/200], Loss: 0.0636, Accuracy: 0.9800, Precision: 0.9800, Recall: 0.9800, F1 Score: 0.9800\n",
      "Epoch [28/200], Loss: 0.0581, Accuracy: 0.9810, Precision: 0.9810, Recall: 0.9810, F1 Score: 0.9809\n",
      "Epoch [29/200], Loss: 0.0650, Accuracy: 0.9805, Precision: 0.9805, Recall: 0.9805, F1 Score: 0.9805\n",
      "Epoch [30/200], Loss: 0.0699, Accuracy: 0.9786, Precision: 0.9787, Recall: 0.9786, F1 Score: 0.9786\n",
      "Epoch [31/200], Loss: 0.0548, Accuracy: 0.9838, Precision: 0.9838, Recall: 0.9838, F1 Score: 0.9838\n",
      "Epoch [32/200], Loss: 0.0644, Accuracy: 0.9752, Precision: 0.9752, Recall: 0.9752, F1 Score: 0.9752\n",
      "Epoch [33/200], Loss: 0.0516, Accuracy: 0.9852, Precision: 0.9852, Recall: 0.9852, F1 Score: 0.9852\n",
      "Epoch [34/200], Loss: 0.0494, Accuracy: 0.9833, Precision: 0.9834, Recall: 0.9833, F1 Score: 0.9834\n",
      "Epoch [35/200], Loss: 0.0590, Accuracy: 0.9786, Precision: 0.9786, Recall: 0.9786, F1 Score: 0.9786\n",
      "Epoch [36/200], Loss: 0.0618, Accuracy: 0.9781, Precision: 0.9781, Recall: 0.9781, F1 Score: 0.9780\n",
      "Epoch [37/200], Loss: 0.0703, Accuracy: 0.9771, Precision: 0.9771, Recall: 0.9771, F1 Score: 0.9771\n",
      "Epoch [38/200], Loss: 0.0546, Accuracy: 0.9795, Precision: 0.9796, Recall: 0.9795, F1 Score: 0.9796\n",
      "Epoch [39/200], Loss: 0.0564, Accuracy: 0.9833, Precision: 0.9834, Recall: 0.9833, F1 Score: 0.9833\n",
      "Epoch [40/200], Loss: 0.0378, Accuracy: 0.9876, Precision: 0.9876, Recall: 0.9876, F1 Score: 0.9876\n",
      "Epoch [41/200], Loss: 0.0684, Accuracy: 0.9795, Precision: 0.9796, Recall: 0.9795, F1 Score: 0.9795\n",
      "Epoch [42/200], Loss: 0.0611, Accuracy: 0.9814, Precision: 0.9814, Recall: 0.9814, F1 Score: 0.9814\n",
      "Epoch [43/200], Loss: 0.0571, Accuracy: 0.9805, Precision: 0.9805, Recall: 0.9805, F1 Score: 0.9804\n",
      "Epoch [44/200], Loss: 0.0437, Accuracy: 0.9829, Precision: 0.9829, Recall: 0.9829, F1 Score: 0.9829\n",
      "Epoch [45/200], Loss: 0.0421, Accuracy: 0.9852, Precision: 0.9853, Recall: 0.9852, F1 Score: 0.9852\n",
      "Epoch [46/200], Loss: 0.0450, Accuracy: 0.9862, Precision: 0.9862, Recall: 0.9862, F1 Score: 0.9862\n",
      "Epoch [47/200], Loss: 0.0503, Accuracy: 0.9810, Precision: 0.9811, Recall: 0.9810, F1 Score: 0.9810\n",
      "Epoch [48/200], Loss: 0.0604, Accuracy: 0.9829, Precision: 0.9829, Recall: 0.9829, F1 Score: 0.9828\n",
      "Epoch [49/200], Loss: 0.0319, Accuracy: 0.9895, Precision: 0.9895, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [50/200], Loss: 0.0452, Accuracy: 0.9867, Precision: 0.9867, Recall: 0.9867, F1 Score: 0.9867\n",
      "Epoch [51/200], Loss: 0.0439, Accuracy: 0.9848, Precision: 0.9848, Recall: 0.9848, F1 Score: 0.9848\n",
      "Epoch [52/200], Loss: 0.0382, Accuracy: 0.9895, Precision: 0.9895, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [53/200], Loss: 0.0328, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [54/200], Loss: 0.0410, Accuracy: 0.9843, Precision: 0.9843, Recall: 0.9843, F1 Score: 0.9843\n",
      "Epoch [55/200], Loss: 0.0296, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [56/200], Loss: 0.0380, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [57/200], Loss: 0.0227, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [58/200], Loss: 0.0337, Accuracy: 0.9895, Precision: 0.9895, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [59/200], Loss: 0.0366, Accuracy: 0.9876, Precision: 0.9877, Recall: 0.9876, F1 Score: 0.9876\n",
      "Epoch [60/200], Loss: 0.0384, Accuracy: 0.9852, Precision: 0.9852, Recall: 0.9852, F1 Score: 0.9852\n",
      "Epoch [61/200], Loss: 0.0336, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [62/200], Loss: 0.0470, Accuracy: 0.9867, Precision: 0.9867, Recall: 0.9867, F1 Score: 0.9867\n",
      "Epoch [63/200], Loss: 0.0325, Accuracy: 0.9881, Precision: 0.9881, Recall: 0.9881, F1 Score: 0.9881\n",
      "Epoch [64/200], Loss: 0.0267, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [65/200], Loss: 0.0365, Accuracy: 0.9871, Precision: 0.9872, Recall: 0.9871, F1 Score: 0.9871\n",
      "Epoch [66/200], Loss: 0.0294, Accuracy: 0.9910, Precision: 0.9911, Recall: 0.9910, F1 Score: 0.9909\n",
      "Epoch [67/200], Loss: 0.0379, Accuracy: 0.9871, Precision: 0.9872, Recall: 0.9871, F1 Score: 0.9871\n",
      "Epoch [68/200], Loss: 0.0423, Accuracy: 0.9862, Precision: 0.9862, Recall: 0.9862, F1 Score: 0.9862\n",
      "Epoch [69/200], Loss: 0.0488, Accuracy: 0.9852, Precision: 0.9854, Recall: 0.9852, F1 Score: 0.9853\n",
      "Epoch [70/200], Loss: 0.0355, Accuracy: 0.9867, Precision: 0.9867, Recall: 0.9867, F1 Score: 0.9867\n",
      "Epoch [71/200], Loss: 0.0320, Accuracy: 0.9910, Precision: 0.9909, Recall: 0.9910, F1 Score: 0.9909\n",
      "Epoch [72/200], Loss: 0.0323, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [73/200], Loss: 0.0321, Accuracy: 0.9890, Precision: 0.9891, Recall: 0.9890, F1 Score: 0.9891\n",
      "Epoch [74/200], Loss: 0.0366, Accuracy: 0.9871, Precision: 0.9872, Recall: 0.9871, F1 Score: 0.9872\n",
      "Epoch [75/200], Loss: 0.0266, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [76/200], Loss: 0.0287, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [77/200], Loss: 0.0337, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [78/200], Loss: 0.0222, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [79/200], Loss: 0.0263, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [80/200], Loss: 0.0217, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [81/200], Loss: 0.0314, Accuracy: 0.9890, Precision: 0.9891, Recall: 0.9890, F1 Score: 0.9891\n",
      "Epoch [82/200], Loss: 0.0283, Accuracy: 0.9905, Precision: 0.9906, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [83/200], Loss: 0.0188, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [84/200], Loss: 0.0189, Accuracy: 0.9933, Precision: 0.9933, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [85/200], Loss: 0.0236, Accuracy: 0.9924, Precision: 0.9924, Recall: 0.9924, F1 Score: 0.9924\n",
      "Epoch [86/200], Loss: 0.0202, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [87/200], Loss: 0.0294, Accuracy: 0.9910, Precision: 0.9910, Recall: 0.9910, F1 Score: 0.9910\n",
      "Epoch [88/200], Loss: 0.0268, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [89/200], Loss: 0.0253, Accuracy: 0.9924, Precision: 0.9925, Recall: 0.9924, F1 Score: 0.9924\n",
      "Epoch [90/200], Loss: 0.0229, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [91/200], Loss: 0.0215, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [92/200], Loss: 0.0348, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [93/200], Loss: 0.0371, Accuracy: 0.9895, Precision: 0.9895, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [94/200], Loss: 0.0352, Accuracy: 0.9886, Precision: 0.9887, Recall: 0.9886, F1 Score: 0.9886\n",
      "Epoch [95/200], Loss: 0.0280, Accuracy: 0.9924, Precision: 0.9924, Recall: 0.9924, F1 Score: 0.9924\n",
      "Epoch [96/200], Loss: 0.0204, Accuracy: 0.9910, Precision: 0.9910, Recall: 0.9910, F1 Score: 0.9910\n",
      "Epoch [97/200], Loss: 0.0303, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [98/200], Loss: 0.0427, Accuracy: 0.9871, Precision: 0.9871, Recall: 0.9871, F1 Score: 0.9871\n",
      "Epoch [99/200], Loss: 0.0185, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [100/200], Loss: 0.0318, Accuracy: 0.9876, Precision: 0.9876, Recall: 0.9876, F1 Score: 0.9876\n",
      "Epoch [101/200], Loss: 0.0320, Accuracy: 0.9881, Precision: 0.9881, Recall: 0.9881, F1 Score: 0.9881\n",
      "Epoch [102/200], Loss: 0.0363, Accuracy: 0.9910, Precision: 0.9910, Recall: 0.9910, F1 Score: 0.9909\n",
      "Epoch [103/200], Loss: 0.0378, Accuracy: 0.9871, Precision: 0.9872, Recall: 0.9871, F1 Score: 0.9871\n",
      "Epoch [104/200], Loss: 0.0391, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [105/200], Loss: 0.0364, Accuracy: 0.9871, Precision: 0.9871, Recall: 0.9871, F1 Score: 0.9871\n",
      "Epoch [106/200], Loss: 0.0342, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [107/200], Loss: 0.0210, Accuracy: 0.9905, Precision: 0.9906, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [108/200], Loss: 0.0192, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9928\n",
      "Epoch [109/200], Loss: 0.0176, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [110/200], Loss: 0.0162, Accuracy: 0.9938, Precision: 0.9939, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [111/200], Loss: 0.0334, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [112/200], Loss: 0.0200, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [113/200], Loss: 0.0360, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [114/200], Loss: 0.0415, Accuracy: 0.9881, Precision: 0.9881, Recall: 0.9881, F1 Score: 0.9881\n",
      "Epoch [115/200], Loss: 0.0263, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [116/200], Loss: 0.0212, Accuracy: 0.9933, Precision: 0.9933, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [117/200], Loss: 0.0311, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [118/200], Loss: 0.0224, Accuracy: 0.9919, Precision: 0.9920, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [119/200], Loss: 0.0349, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [120/200], Loss: 0.0260, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [121/200], Loss: 0.0361, Accuracy: 0.9881, Precision: 0.9881, Recall: 0.9881, F1 Score: 0.9881\n",
      "Epoch [122/200], Loss: 0.0273, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [123/200], Loss: 0.0299, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [124/200], Loss: 0.0257, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [125/200], Loss: 0.0303, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [126/200], Loss: 0.0189, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [127/200], Loss: 0.0275, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [128/200], Loss: 0.0275, Accuracy: 0.9895, Precision: 0.9895, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [129/200], Loss: 0.0195, Accuracy: 0.9933, Precision: 0.9934, Recall: 0.9933, F1 Score: 0.9934\n",
      "Epoch [130/200], Loss: 0.0272, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [131/200], Loss: 0.0311, Accuracy: 0.9910, Precision: 0.9911, Recall: 0.9910, F1 Score: 0.9910\n",
      "Epoch [132/200], Loss: 0.0194, Accuracy: 0.9924, Precision: 0.9924, Recall: 0.9924, F1 Score: 0.9924\n",
      "Epoch [133/200], Loss: 0.0213, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [134/200], Loss: 0.0290, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [135/200], Loss: 0.0246, Accuracy: 0.9933, Precision: 0.9933, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [136/200], Loss: 0.0135, Accuracy: 0.9962, Precision: 0.9962, Recall: 0.9962, F1 Score: 0.9962\n",
      "Epoch [137/200], Loss: 0.0303, Accuracy: 0.9900, Precision: 0.9902, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [138/200], Loss: 0.0257, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9928\n",
      "Epoch [139/200], Loss: 0.0207, Accuracy: 0.9933, Precision: 0.9934, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [140/200], Loss: 0.0168, Accuracy: 0.9919, Precision: 0.9920, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [141/200], Loss: 0.0335, Accuracy: 0.9914, Precision: 0.9914, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [142/200], Loss: 0.0296, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [143/200], Loss: 0.0263, Accuracy: 0.9900, Precision: 0.9900, Recall: 0.9900, F1 Score: 0.9900\n",
      "Epoch [144/200], Loss: 0.0220, Accuracy: 0.9938, Precision: 0.9939, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [145/200], Loss: 0.0230, Accuracy: 0.9895, Precision: 0.9896, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [146/200], Loss: 0.0225, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [147/200], Loss: 0.0273, Accuracy: 0.9886, Precision: 0.9887, Recall: 0.9886, F1 Score: 0.9886\n",
      "Epoch [148/200], Loss: 0.0191, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [149/200], Loss: 0.0166, Accuracy: 0.9962, Precision: 0.9962, Recall: 0.9962, F1 Score: 0.9962\n",
      "Epoch [150/200], Loss: 0.0190, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [151/200], Loss: 0.0187, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [152/200], Loss: 0.0272, Accuracy: 0.9933, Precision: 0.9934, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [153/200], Loss: 0.0235, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [154/200], Loss: 0.0290, Accuracy: 0.9919, Precision: 0.9920, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [155/200], Loss: 0.0222, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [156/200], Loss: 0.0209, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [157/200], Loss: 0.0292, Accuracy: 0.9895, Precision: 0.9896, Recall: 0.9895, F1 Score: 0.9895\n",
      "Epoch [158/200], Loss: 0.0252, Accuracy: 0.9919, Precision: 0.9919, Recall: 0.9919, F1 Score: 0.9919\n",
      "Epoch [159/200], Loss: 0.0284, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [160/200], Loss: 0.0154, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [161/200], Loss: 0.0101, Accuracy: 0.9957, Precision: 0.9957, Recall: 0.9957, F1 Score: 0.9957\n",
      "Epoch [162/200], Loss: 0.0144, Accuracy: 0.9952, Precision: 0.9952, Recall: 0.9952, F1 Score: 0.9952\n",
      "Epoch [163/200], Loss: 0.0182, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [164/200], Loss: 0.0200, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [165/200], Loss: 0.0167, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [166/200], Loss: 0.0175, Accuracy: 0.9957, Precision: 0.9957, Recall: 0.9957, F1 Score: 0.9957\n",
      "Epoch [167/200], Loss: 0.0338, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [168/200], Loss: 0.0151, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [169/200], Loss: 0.0185, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [170/200], Loss: 0.0261, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [171/200], Loss: 0.0151, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [172/200], Loss: 0.0221, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [173/200], Loss: 0.0266, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [174/200], Loss: 0.0233, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [175/200], Loss: 0.0296, Accuracy: 0.9910, Precision: 0.9910, Recall: 0.9910, F1 Score: 0.9910\n",
      "Epoch [176/200], Loss: 0.0246, Accuracy: 0.9914, Precision: 0.9915, Recall: 0.9914, F1 Score: 0.9914\n",
      "Epoch [177/200], Loss: 0.0202, Accuracy: 0.9933, Precision: 0.9934, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [178/200], Loss: 0.0161, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [179/200], Loss: 0.0100, Accuracy: 0.9971, Precision: 0.9971, Recall: 0.9971, F1 Score: 0.9971\n",
      "Epoch [180/200], Loss: 0.0160, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [181/200], Loss: 0.0196, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [182/200], Loss: 0.0246, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [183/200], Loss: 0.0131, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [184/200], Loss: 0.0222, Accuracy: 0.9933, Precision: 0.9934, Recall: 0.9933, F1 Score: 0.9933\n",
      "Epoch [185/200], Loss: 0.0128, Accuracy: 0.9943, Precision: 0.9943, Recall: 0.9943, F1 Score: 0.9943\n",
      "Epoch [186/200], Loss: 0.0103, Accuracy: 0.9967, Precision: 0.9967, Recall: 0.9967, F1 Score: 0.9967\n",
      "Epoch [187/200], Loss: 0.0107, Accuracy: 0.9967, Precision: 0.9967, Recall: 0.9967, F1 Score: 0.9967\n",
      "Epoch [188/200], Loss: 0.0093, Accuracy: 0.9962, Precision: 0.9962, Recall: 0.9962, F1 Score: 0.9962\n",
      "Epoch [189/200], Loss: 0.0200, Accuracy: 0.9957, Precision: 0.9957, Recall: 0.9957, F1 Score: 0.9957\n",
      "Epoch [190/200], Loss: 0.0127, Accuracy: 0.9967, Precision: 0.9967, Recall: 0.9967, F1 Score: 0.9967\n",
      "Epoch [191/200], Loss: 0.0172, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9948, F1 Score: 0.9948\n",
      "Epoch [192/200], Loss: 0.0171, Accuracy: 0.9952, Precision: 0.9953, Recall: 0.9952, F1 Score: 0.9952\n",
      "Epoch [193/200], Loss: 0.0243, Accuracy: 0.9924, Precision: 0.9924, Recall: 0.9924, F1 Score: 0.9924\n",
      "Epoch [194/200], Loss: 0.0108, Accuracy: 0.9967, Precision: 0.9967, Recall: 0.9967, F1 Score: 0.9967\n",
      "Epoch [195/200], Loss: 0.0161, Accuracy: 0.9962, Precision: 0.9962, Recall: 0.9962, F1 Score: 0.9962\n",
      "Epoch [196/200], Loss: 0.0388, Accuracy: 0.9905, Precision: 0.9905, Recall: 0.9905, F1 Score: 0.9905\n",
      "Epoch [197/200], Loss: 0.0160, Accuracy: 0.9938, Precision: 0.9938, Recall: 0.9938, F1 Score: 0.9938\n",
      "Epoch [198/200], Loss: 0.0188, Accuracy: 0.9929, Precision: 0.9929, Recall: 0.9929, F1 Score: 0.9929\n",
      "Epoch [199/200], Loss: 0.0157, Accuracy: 0.9952, Precision: 0.9953, Recall: 0.9952, F1 Score: 0.9952\n",
      "Epoch [200/200], Loss: 0.0147, Accuracy: 0.9971, Precision: 0.9972, Recall: 0.9971, F1 Score: 0.9971\n",
      "Accuracy of the model on the test data: 0.87%\n",
      "Precision: 0.8758, Recall: 0.8743, F1 Score: 0.8743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"IEEG_Classification_Baseline\"\n",
    "\n",
    "# Create a new experiment or set the existing one\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = 3035  # sequence length\n",
    "num_classes = len(dataset.label_encoder.classes_)\n",
    "model = SimpleNN(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs=200):\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"ANN-Baseline\") as run:\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"epochs\", num_epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", 0.001)\n",
    "        mlflow.log_param(\"model\", \"SimpleNN\")\n",
    "        mlflow.log_param(\"input_size\", input_size)\n",
    "        mlflow.log_param(\"num_classes\", num_classes)\n",
    "        mlflow.log_dict(dataset.get_class_mapping(), \"class_mapping.json\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            y_true_train = []\n",
    "            y_pred_train = []\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true_train.extend(labels.squeeze().cpu().numpy())\n",
    "                y_pred_train.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_true_train, y_pred_train, average='weighted')\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "            \n",
    "            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"train_precision\", precision, step=epoch)\n",
    "            mlflow.log_metric(\"train_recall\", recall, step=epoch)\n",
    "            mlflow.log_metric(\"train_f1\", f1, step=epoch)\n",
    "\n",
    "        model.eval()\n",
    "        y_true_test = []\n",
    "        y_pred_test = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true_test.extend(labels.squeeze().cpu().numpy())\n",
    "                y_pred_test.extend(predicted.cpu().numpy())\n",
    "                \n",
    "        test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average='weighted')\n",
    "        \n",
    "        print(f'Accuracy of the model on the test data: {test_accuracy:.2f}%')\n",
    "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "        \n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", precision)\n",
    "        mlflow.log_metric(\"test_recall\", recall)\n",
    "        mlflow.log_metric(\"test_f1\", f1)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "        cm_df = pd.DataFrame(cm, index=dataset.label_encoder.classes_, columns=dataset.label_encoder.classes_)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "# Train and evaluate the model while tracking with MLflow\n",
    "train_and_evaluate(model, train_loader, test_loader, num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
